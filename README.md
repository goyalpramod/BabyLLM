# BabyLLM

A small LLM built at home, to beat GPT-2. We will together learn to beat a model built by big cord in {} days trained on {} GPUs which roughly costed them {} dollars. 

Our constraints: 

- It should acheive equal or better performance in atleast 3 benchmarks that gpt-2 was evaluated on 
- Same dataset (Everyone knows garbage in garbage out, so I believe it will be cheating if we use a better dataset)
- It can be trained in a reasonable amount of time. Less than a week. 
- It must be on a SINGLE GPU. (I am using a RTX 4090)
- {add anything more relevant}

We will make changes to architecture, optimize training and just go ham. So let us begin.

## Dataset 

GPT was trained on {} data, we will use that (to keep the competition atleast a bit fair haha) and also try [FineWeb]() (Cause let's see how crazy we can get)

### Filtering

We can train on the same data, but filtering and improving it will make our model better. And I do not consider this cheating as we are working with the same data

### Data Cleaning 

## Scraper 

I found the idea of getting so much data very fascinating, so here is my replication of some scrapers (think of it as more or less as guide to understanding how scrapers work and how you can write your own)


## Tokenization 

They used BPE, we can use that or SentencePiece... let's experiment

## Embedding 

## Architecture 

## Pre-training 

## Fine-Tuning 

## RLHF 


## References
[Cite properly]

- https://blog.briankitano.com/llama-from-scratch/
- https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch